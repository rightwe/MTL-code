{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建网络模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scaled_dot_product_attention(nn.Module):\n",
    "\n",
    "    def __init__(self, att_dropout=0.0):\n",
    "        super(scaled_dot_product_attention, self).__init__()\n",
    "        self.dropout = nn.Dropout(att_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, scale=None):\n",
    "        '''\n",
    "        args:\n",
    "            q: [batch_size, q_length, q_dimension]\n",
    "            k: [batch_size, k_length, k_dimension]\n",
    "            v: [batch_size, v_length, v_dimension]\n",
    "            q_dimension = k_dimension = v_dimension\n",
    "            scale: 缩放因子\n",
    "        return:\n",
    "            attention, alpha\n",
    "        '''\n",
    "        # 快使用神奇的爱因斯坦求和约定吧！\n",
    "        alpha = torch.einsum('ijk,ilk->ijl', [q, k])# query和key向量相乘\n",
    "        if scale:\n",
    "            alpha = alpha * scale\n",
    "        alpha = self.softmax(alpha)\n",
    "        alpha = self.dropout(alpha)\n",
    "        attention = torch.einsum('ijl,ilk->ijk', [alpha, v])\n",
    "        return attention, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        '''\n",
    "        input_dim:输入特征的维度\n",
    "        hidden_dim:隐藏层维度\n",
    "        query_dim:query向量的维度,因为其直接由隐藏层输出,故query_dim = output_dim\n",
    "        此网络模块的最终输出是query向量\n",
    "        '''\n",
    "        super(TaskNet, self).__init__()\n",
    "        self.query_dim = hidden_dim\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self,X):\n",
    "        query = self.fc1(X)\n",
    "        return query\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        '''\n",
    "        input_dim:输入特征的维度\n",
    "        hidden_dim:隐藏层维度\n",
    "        key_dim:key向量的维度,因为其直接由隐藏层输出,故key_dim = output_dim\n",
    "        此网络模块的最终输出是key向量\n",
    "        '''\n",
    "        super(Expert, self).__init__()\n",
    "        self.key_dim = hidden_dim\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self,X):\n",
    "        key = self.fc1(X)\n",
    "        return key\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tower(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super(Tower, self).__init__()\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16,output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.fc1(X)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=18\n",
    "X = torch.ones(input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拼接网络模块，构建完整网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AOE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, task_num, expert_num):\n",
    "        super(AOE, self).__init__()\n",
    "        # model\n",
    "        self.tasknet_list = nn.ModuleList([TaskNet(18,64) for i in range(task_num)])\n",
    "        self.expert_list = nn.ModuleList([Expert(18,64) for i in range(expert_num)])\n",
    "        self.tower_list = nn.ModuleList([Tower(64,1) for i in range(task_num)])\n",
    "        # vector\n",
    "        self.query = []\n",
    "        self.key = []\n",
    "        self.value = []\n",
    "        self.alpha = []\n",
    "        self.attention = []\n",
    "        # function\n",
    "        self.softmax = nn.Softmax(dim=0)# dim=0代表作用于行向量，dim=1代表作用于列向量\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        self.query = [task_net(X) for task_net in self.tasknet_list]\n",
    "        self.query = torch.stack(self.query)\n",
    "\n",
    "        self.key = [expert(X) for expert in self.expert_list]\n",
    "        self.key = torch.stack(self.key)  #shape:torch.Size([5, 64])\n",
    "\n",
    "        for q in self.query:\n",
    "            a = [(q@k.T) for k in self.key]\n",
    "            a = self.softmax(torch.tensor(a))\n",
    "            self.alpha.append(a)\n",
    "\n",
    "        self.value = self.key.clone()\n",
    "        \n",
    "        for a in self.alpha:\n",
    "            a.view(1,5)\n",
    "            self.attention.append(a@self.value)\n",
    "        self.attention = torch.cat(self.attention, dim=0).reshape(4,64) #将attention列表转为tensor类型\n",
    "\n",
    "        # 拼接attention值和TaskNet输出值，然后送入Tower\n",
    "        # 其实TaskNet的输出值就是query\n",
    "        query_add_attention = self.query + self.attention\n",
    "\n",
    "        ## 传入TOWER\n",
    "        tower_input = query_add_attention\n",
    "        tower_output = [tower(x) for tower, x in zip(self.tower_list, tower_input)]\n",
    "\n",
    "        return tower_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=18\n",
    "X = torch.ones(input_dim)\n",
    "model = AOE(18, 64, 1, 4, 5)\n",
    "o = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.0589], grad_fn=<AddBackward0>),\n",
       " tensor([-0.1372], grad_fn=<AddBackward0>),\n",
       " tensor([-0.2355], grad_fn=<AddBackward0>),\n",
       " tensor([0.1954], grad_fn=<AddBackward0>)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建q,k,v矩阵，且qkv三个矩阵不参与梯度更新，因此要with torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建query矩阵\n",
    "'''\n",
    "1. 确定任务数量\n",
    "2. 初始化task_num个网络,可存入了列表\n",
    "3. 将每个网络输出的query向量拼接起来\n",
    "'''\n",
    "task_num = 4\n",
    "tasknet_list = nn.ModuleList([TaskNet(18,64) for i in range(task_num)])\n",
    "query = [task_net(X) for task_net in tasknet_list]\n",
    "query = torch.stack(query) # shape:torch.Size([4, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建key矩阵\n",
    "'''\n",
    "1. 确定专家网络数量\n",
    "2. 初始化expert_num个网络\n",
    "3. 拼接每个专家网络输出的key向量\n",
    "'''\n",
    "expert_num = 5\n",
    "expert_list = nn.ModuleList([Expert(18,64) for i in range(expert_num)])\n",
    "key = [expert(X) for expert in expert_list]\n",
    "key = torch.stack(key)  #shape:torch.Size([5, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2252, 0.1835, 0.2419, 0.1672, 0.1822],\n",
       "        [0.2161, 0.2186, 0.2349, 0.1696, 0.1607],\n",
       "        [0.2128, 0.2158, 0.1943, 0.2018, 0.1754],\n",
       "        [0.2130, 0.1962, 0.1929, 0.2198, 0.1780]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建alpha矩阵\n",
    "'''\n",
    "1. 针对每个任务，学得一个权重向量(0.2, 0.3, 0.1 .... 0.1)\n",
    "2. 多个任务所对应的权重向量拼接成一个权重矩阵\n",
    "'''\n",
    "softmax = nn.Softmax(dim=0) # dim=0代表作用于行向量，dim=1代表作用于列向量\n",
    "alpha = []\n",
    "for q in query:\n",
    "    a = [(q@k.T) for k in key]\n",
    "    a = softmax(torch.tensor(a))\n",
    "    alpha.append(a)\n",
    "\n",
    "alpha = torch.stack(alpha)\n",
    "alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建value矩阵\n",
    "value = key.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算attention值\n",
    "'''\n",
    "1. alpha[0]:[0.2104, 0.1306, 0.2965, 0.1911, 0.1713]\n",
    "   针对任务A来说,这五个值分别代表五个专家网络所占的权重\n",
    "   针对任务B来说,则需要查看alpha[1]\n",
    "\n",
    "2. value矩阵的形状为[5, 64],5代表5个专家网络, 64代表专家网络的输出维度\n",
    "\n",
    "3. 若想输出针对任务A的attention值,则\n",
    "'''\n",
    "attention = []\n",
    "for a in alpha:\n",
    "    a.view(1,5)\n",
    "    attention.append(a@value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 4],\n",
       "        [6, 8]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = torch.tensor([1,2,3,4]).reshape(2,2)\n",
    "m2 = torch.tensor([1,2,3,4]).reshape(2,2)\n",
    "m1+m2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('First')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b3391400c782768846f32ba12f709f10c798f831f54432f6435a679695d9de8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
