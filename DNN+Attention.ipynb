{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建网络模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_file import read_file, getTensorDataset\n",
    "X, y = read_file(filepath='./4tasks-encode.xlsx', iScaler=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.astype(np.float32), y, test_size=0.33, random_state=25)\n",
    "train_loader = DataLoader(dataset=getTensorDataset(X_train, y_train), batch_size=54)\n",
    "val_loader = DataLoader(dataset=getTensorDataset(X_test,y_test),batch_size=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scaled_dot_product_attention(nn.Module):\n",
    "\n",
    "    def __init__(self, att_dropout=0.0):\n",
    "        super(scaled_dot_product_attention, self).__init__()\n",
    "        self.dropout = nn.Dropout(att_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, scale=None):\n",
    "        '''\n",
    "        args:\n",
    "            q: [batch_size, q_length, q_dimension]\n",
    "            k: [batch_size, k_length, k_dimension]\n",
    "            v: [batch_size, v_length, v_dimension]\n",
    "            q_dimension = k_dimension = v_dimension\n",
    "            scale: 缩放因子\n",
    "        return:\n",
    "            attention, alpha\n",
    "        '''\n",
    "        # 快使用神奇的爱因斯坦求和约定吧！\n",
    "        alpha = torch.einsum('ijk,ilk->ijl', [q, k])# query和key向量相乘\n",
    "        if scale:\n",
    "            alpha = alpha * scale\n",
    "        alpha = self.softmax(alpha)\n",
    "        alpha = self.dropout(alpha)\n",
    "        attention = torch.einsum('ijl,ilk->ijk', [alpha, v])\n",
    "        return attention, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        '''\n",
    "        input_dim:输入特征的维度\n",
    "        hidden_dim:隐藏层维度\n",
    "        query_dim:query向量的维度,因为其直接由隐藏层输出,故query_dim = output_dim\n",
    "        此网络模块的最终输出是query向量\n",
    "        '''\n",
    "        super(TaskNet, self).__init__()\n",
    "        self.query_dim = hidden_dim\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self,X):\n",
    "        query = self.fc1(X)\n",
    "        return query\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasknet = TaskNet(18,64)\n",
    "for train_X, train_y in train_loader:\n",
    "    out = tasknet(train_X)\n",
    "    print(out.shape)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        '''\n",
    "        input_dim:输入特征的维度\n",
    "        hidden_dim:隐藏层维度\n",
    "        key_dim:key向量的维度,因为其直接由隐藏层输出,故key_dim = output_dim\n",
    "        此网络模块的最终输出是key向量\n",
    "        '''\n",
    "        super(Expert, self).__init__()\n",
    "        self.key_dim = hidden_dim\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self,X):\n",
    "        key = self.fc1(X)\n",
    "        return key\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([54, 54])\n"
     ]
    }
   ],
   "source": [
    "tasknet = TaskNet(18,64)\n",
    "exp = Expert(18,64)\n",
    "for train_X, train_y in train_loader:\n",
    "    query = tasknet(train_X)\n",
    "    key = exp(train_X)\n",
    "    alpha = query@key.T\n",
    "    print(alpha.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tower(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super(Tower, self).__init__()\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16,output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.fc1(X)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拼接网络模块，构建完整网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AOE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, task_num, expert_num):\n",
    "        super(AOE, self).__init__()\n",
    "        # model\n",
    "        self.tasknet_list = nn.ModuleList([TaskNet(18,64) for i in range(task_num)])\n",
    "        self.expert_list = nn.ModuleList([Expert(18,64) for i in range(expert_num)])\n",
    "        self.tower_list = nn.ModuleList([Tower(64,1) for i in range(task_num)])\n",
    "        # vector\n",
    "        self.query = []\n",
    "        self.key = []\n",
    "        self.value = []\n",
    "        self.alpha = []\n",
    "        self.attention = []\n",
    "        # function\n",
    "        self.softmax = nn.Softmax(dim=1)# dim=0代表作用于行向量，dim=1代表作用于列向量\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        # 构建query\n",
    "        self.query = [task_net(X) for task_net in self.tasknet_list]\n",
    "        self.query = torch.stack(self.query)\n",
    "\n",
    "        # 构建key\n",
    "        self.key = [expert(X) for expert in self.expert_list]\n",
    "        self.key = torch.stack(self.key)  #shape:torch.Size([5, 64])\n",
    "\n",
    "        # 计算权重alpha\n",
    "        for q in self.query:\n",
    "            a = [(q@k.T) for k in self.key]\n",
    "            print(a[1].shape)\n",
    "            a = self.softmax(torch.tensor(a))\n",
    "            self.alpha.append(a)\n",
    "\n",
    "        # 构建value\n",
    "        self.value = self.key.clone()\n",
    "        \n",
    "        # 计算attention值\n",
    "        for a in self.alpha:\n",
    "            a.view(1,5)\n",
    "            self.attention.append(a@self.value)\n",
    "        self.attention = torch.cat(self.attention, dim=0).reshape(4,64) #将attention列表转为tensor类型\n",
    "\n",
    "        # 拼接attention值和TaskNet输出值，然后送入Tower\n",
    "        # 其实TaskNet的输出值就是query\n",
    "        query_add_attention = self.query + self.attention\n",
    "\n",
    "        ## 传入TOWER\n",
    "        tower_input = query_add_attention\n",
    "        tower_output = [tower(ti) for tower, ti in zip(self.tower_list, tower_input)]\n",
    "\n",
    "        return tower_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据，构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_file import read_file, getTensorDataset\n",
    "X, y = read_file(filepath='./4tasks-encode.xlsx', iScaler=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.astype(np.float32), y, test_size=0.33, random_state=25)\n",
    "train_loader = DataLoader(dataset=getTensorDataset(X_train, y_train), batch_size=54)\n",
    "val_loader = DataLoader(dataset=getTensorDataset(X_test,y_test),batch_size=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\How\\Desktop\\Experiments\\Attention\\DNN+Attention.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/How/Desktop/Experiments/Attention/DNN%2BAttention.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m AOE(\u001b[39m18\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/How/Desktop/Experiments/Attention/DNN%2BAttention.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m train_X, train_y \u001b[39min\u001b[39;00m val_loader:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/How/Desktop/Experiments/Attention/DNN%2BAttention.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     out \u001b[39m=\u001b[39m model(train_X)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/How/Desktop/Experiments/Attention/DNN%2BAttention.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(out)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/How/Desktop/Experiments/Attention/DNN%2BAttention.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32me:\\Users\\How\\anaconda3\\envs\\First\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\How\\Desktop\\Experiments\\Attention\\DNN+Attention.ipynb Cell 12\u001b[0m in \u001b[0;36mAOE.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/How/Desktop/Experiments/Attention/DNN%2BAttention.ipynb#X30sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39m# print(a[1].shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/How/Desktop/Experiments/Attention/DNN%2BAttention.ipynb#X30sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39m# a = self.softmax(torch.tensor(a))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/How/Desktop/Experiments/Attention/DNN%2BAttention.ipynb#X30sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha\u001b[39m.\u001b[39mappend(a)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/How/Desktop/Experiments/Attention/DNN%2BAttention.ipynb#X30sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malpha, dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/How/Desktop/Experiments/Attention/DNN%2BAttention.ipynb#X30sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/How/Desktop/Experiments/Attention/DNN%2BAttention.ipynb#X30sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# 构建value\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"
     ]
    }
   ],
   "source": [
    "model = AOE(18, 64, 1, 4, 5)\n",
    "for train_X, train_y in val_loader:\n",
    "    out = model(train_X)\n",
    "    print(out)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "out[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建q,k,v矩阵，且qkv三个矩阵不参与梯度更新，因此要with torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_file import read_file, getTensorDataset\n",
    "X, y = read_file(filepath='./4tasks-encode.xlsx', iScaler=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.astype(np.float32), y, test_size=0.33, random_state=25)\n",
    "train_loader = DataLoader(dataset=getTensorDataset(X_train, y_train), batch_size=54)\n",
    "val_loader = DataLoader(dataset=getTensorDataset(X_test,y_test),batch_size=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 54, 64])\n"
     ]
    }
   ],
   "source": [
    "# 构建query矩阵\n",
    "'''\n",
    "1. 确定任务数量\n",
    "2. 初始化task_num个网络,可存入了列表\n",
    "3. 将每个网络输出的query向量拼接起来\n",
    "'''\n",
    "task_num = 4\n",
    "tasknet_list = nn.ModuleList([TaskNet(18,64) for i in range(task_num)])\n",
    "query = []\n",
    "for train_X, train_y in train_loader:\n",
    "    query = [task_net(train_X) for task_net in tasknet_list]\n",
    "    query = torch.stack(query) # shape:torch.Size([4, 64])\n",
    "print(query.shape)  # [task_num, batchsize, hidden_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 54, 64])\n"
     ]
    }
   ],
   "source": [
    "# 构建key矩阵\n",
    "'''\n",
    "1. 确定专家网络数量\n",
    "2. 初始化expert_num个网络\n",
    "3. 拼接每个专家网络输出的key向量\n",
    "'''\n",
    "expert_num = 6\n",
    "expert_list = nn.ModuleList([Expert(18,64) for i in range(expert_num)])\n",
    "key = []\n",
    "for train_X, train_y in train_loader:\n",
    "    key = [expert(train_X) for expert in expert_list]\n",
    "    key = torch.stack(key)  #shape:torch.Size([5, 64])\n",
    "print(key.shape)  # [task_num, batchsize, hidden_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1460, 0.1452, 0.1774, 0.1804, 0.1597, 0.1914],\n",
       "        [0.1646, 0.1623, 0.1839, 0.1578, 0.1667, 0.1647],\n",
       "        [0.1659, 0.1582, 0.1660, 0.1759, 0.1710, 0.1631],\n",
       "        [0.1619, 0.1561, 0.1691, 0.1730, 0.1633, 0.1766]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 计算alpha矩阵的第二种方法\n",
    "softmax = nn.Softmax(dim=1)\n",
    "alpha = []\n",
    "for q in query:\n",
    "    alpha_temp = []\n",
    "    for k in key:\n",
    "        cos_similarity = cosine_similarity(q, k).mean() # 采用余弦相似度计算行向量之间的相似度（得到54个值， 然后取平均）\n",
    "        alpha_temp.append(cos_similarity)\n",
    "    alpha_temp = torch.stack(alpha_temp)  \n",
    "    print(type(alpha))   \n",
    "    alpha.append(alpha_temp)\n",
    "alpha = torch.stack(alpha)\n",
    "alpha = softmax(alpha)\n",
    "alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([54, 64])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建value矩阵\n",
    "value = key.clone()\n",
    "value[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 54, 64])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = []\n",
    "for a_i in alpha:\n",
    "    # 计算第一个任务关于专家网络的attention值\n",
    "    temp = []\n",
    "    for a_ij, vi in zip(a_i, value):\n",
    "        # 权重和value值分别相乘, 存入列表后求和\n",
    "        attention_i = a_ij*vi\n",
    "        temp.append(attention_i)\n",
    "    attention.append(sum(temp))\n",
    "attention = torch.stack(attention)\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算attention值\n",
    "'''\n",
    "1. alpha[0]:[0.2104, 0.1306, 0.2965, 0.1911, 0.1713]\n",
    "   针对任务A来说,这五个值分别代表五个专家网络所占的权重\n",
    "   针对任务B来说,则需要查看alpha[1]\n",
    "\n",
    "2. value矩阵的形状为[5, 54 64],5代表5个专家网络, 54代表batchsize, 64代表专家网络的输出维度\n",
    "\n",
    "3. 若想输出针对任务A的attention值,则\n",
    "'''\n",
    "attention = []\n",
    "for a in alpha:\n",
    "    a.view(1,5)\n",
    "    attention.append(a@value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 4],\n",
       "        [6, 8]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = torch.tensor([1,2,3,4]).reshape(2,2)\n",
    "m2 = torch.tensor([1,2,3,4]).reshape(2,2)\n",
    "m1+m2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('First')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b3391400c782768846f32ba12f709f10c798f831f54432f6435a679695d9de8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
